version: '3.8'

services:
  # ==============================
  # MinIO (S3-compatible)
  # ==============================
  minio:
    image: minio/minio
    container_name: minio
    ports:
      - "9000:9000"   # API
      - "9001:9001"   # Console UI
    environment:
      MINIO_ROOT_USER:  minio
      MINIO_ROOT_PASSWORD: minio123
    command: server /data --console-address ":9001"
    volumes:
      - minio_data:/data
    networks:
      - common_network
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:9000/minio/health/ready || exit 1"]
      interval: 5s
      timeout: 2s
      retries: 12
    
    # MinIO Client for bucket initialization (waits until MinIO is healthy)
  minio-init:
    image: minio/mc:latest
    container_name: minio-init
    depends_on:
      minio:
        condition: service_healthy
    volumes:
      - ./source:/source
    entrypoint: |
      /bin/sh -c "
      mc alias set myminio http://minio:9000 minio minio123;
      mc mb myminio/landing --ignore-existing;
      mc mb myminio/bronze  --ignore-existing;
      mc mb myminio/silver  --ignore-existing;
      mc mb myminio/gold    --ignore-existing;
      echo 'MinIO buckets created successfully';

      echo 'Uploading files from local source folder...';
      mc cp /source/* myminio/landing/;
      echo 'Local files uploaded to MinIO landing bucket successfully';
      "
    networks:
      - common_network

  # ==============================
  # Spark Master
  # ==============================
  spark-master:
    container_name: spark-master
    build:
      context: .
      dockerfile: DockerFile
    environment:
      - SPARK_MODE=master
    ports:
      - "7077:7077"   # protocole Spark RPC (master)
      - "8080:8080"   # UI web du master
    volumes:
      - ./Architecture_Medaillon:/opt/bitnami/spark/Architecture_Medaillon
    depends_on:
      - minio
    networks:
      - common_network

  # ==============================
  # Spark Worker #1
  # ==============================
  spark-worker-1:
    container_name: spark-worker-1
    build:
      context: .
      dockerfile: DockerFile
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=spark://spark-master:7077
    depends_on:
      - spark-master
    networks:
      - common_network

  # ==============================
  # Spark Worker #2
  # ==============================
  spark-worker-2:
    container_name: spark-worker-2
    build:
      context: .
      dockerfile: DockerFile
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=spark://spark-master:7077
    depends_on:
      - spark-master
    networks:
      - common_network

  # ==============================
  # Airflow (Local Executor)
  # ==============================
  airflow:
    image: apache/airflow:2.8.1
    container_name: airflow
    restart: always
    ports:
      - "8081:8080"   # UI Airflow exposée sur http://localhost:8081
    environment:
      - LOAD_EX=true
      - EXECUTOR=Local
    volumes:
      # L’image Airflow 2.8.1 utilise /opt/airflow comme AIRFLOW_HOME
      - ./dags:/opt/airflow/dags
      - ./logs:/opt/airflow/logs
      - ./plugins:/opt/airflow/plugins
    depends_on:
      - spark-master
      - spark-worker-1
      - spark-worker-2
      - minio
    command: > 
      bash -c "airflow db init && \
               airflow users create --username admin --firstname Admin --lastname User --role Admin --email admin@example.com --password admin123 && \
               airflow scheduler & \
               airflow webserver"
    networks:
      - common_network

volumes:
  minio_data:

networks:
  common_network:
    driver: bridge